\documentclass[webpdf,contemporary,large,namedate]{oup-authoring-template}%

%\PassOptionsToPackage{hyphens}{url}
%\PassOptionsToPackage{colorlinks,linkcolor=blue,urlcolor=blue,citecolor=blue,anchorcolor=blue}{hyperref}

\DeclareMathOperator*{\argmax}{argmax}

\usepackage{algorithmicx}
\usepackage{lmodern}
\usepackage{setspace}
\renewcommand{\ttdefault}{cmtt}

\begin{document}
\journaltitle{TBD}
\DOI{TBD}
\copyrightyear{2025}
\pubyear{2025}
\access{Advance Access Publication Date: Day Month Year}
\appnotes{Preprint}
\firstpage{1}

\title[Finding low-complexity sequences]{Finding low-complexity DNA sequences with longdust}
\author[1,2,3,$\ast$]{Heng Li\ORCID{0000-0003-4874-2874}}
\author[4]{Brian Li}
\address[1]{Department of Biomedical Informatics, Harvard Medical School, 10 Shattuck St, Boston, MA 02215, USA}
\address[2]{Department of Data Science, Dana-Farber Cancer Institute, 450 Brookline Ave, Boston, MA 02215, USA}
\address[3]{Broad Insitute of MIT and Harvard, 415 Main St, Cambridge, MA 02142, USA}
\address[4]{Commonwealth School, Boston, MA 02116, USA}
\corresp[$\ast$]{Corresponding author. \href{mailto:hli@ds.dfci.harvard.edu}{hli@ds.dfci.harvard.edu}}

%\received{Date}{0}{Year}
%\revised{Date}{0}{Year}
%\accepted{Date}{0}{Year}

\abstract{
\sffamily\footnotesize
\textbf{Motivation:}
Low-complexity (LC) DNA sequences are compositionally repetitive sequences
that are often associated with increased variant density and variant calling artifacts.
While algorithms for identifying LC sequences exist,
they either lack rigorous mathematical foundation
or are inefficient with long context windows.
\vspace{0.5em}\\
\textbf{Results:}
Longdust is a new algorithm that efficiently identifies long LC sequences including centromeric satellite
and tandem repeats with moderately long motifs.
It defines string complexity by statistically modeling the $k$-mer count distribution
with the parameters: the $k$-mer length, the context window size and a threshold on complexity.
Longdust exhibits high performance on real data and high consistency with existing methods.
\vspace{0.5em}\\
\textbf{Availability and implementation:}
\url{https://github.com/lh3/longdust}
}

\maketitle

\section{Introduction}

In computer science, a string is of low complexity (LC) if it is repetitive in composition.
LC strings tend to be tandemly repetitive and
most of them can be identified with tandem repeat finding algorithms such as
TRF~\citep{Benson:1999aa}, TANTAN~\citep{Frith:2011aa}, ULTRA~\citep{Olson:2024aa} and pytrf~\citep{Du:2025aa}.
These algorithms do not rigorously define string complexity.
They rely on heuristics to search for impure tandem repeats
and cannot identify LC strings without clear tandem structures.
SDUST~\citep{Morgulis:2006aa} is the only widely used algorithm that explicitly defines string complexity
and finds the exact solutions.
However, with $O(w^3L)$ time complexity, where $w$ is the window size and $L$ is the genome length,
SDUST is inefficient given a large $w$ and is thus impractical for finding satellite or tandem repeats with long motifs.
Furthermore, the complexity scoring function used by SDUST is not backed by rigorous modeling.
The theoretical properties of LC strings defined this way are unclear.

Inspired by SDUST, we sought an alternative way to define the $k$-mer complexity of a string
and to identify LC regions in a genome.
Our complexity scoring function is based on a statistical model of $k$-mer count distribution
and our algorithm is practically close to $O(wL)$ in time complexity,
enabling the efficient identification of LC strings in long context windows.

\section{Methods}

Similar to SDUST~\citep{Morgulis:2006aa}, we define the complexity of a DNA string with a function of the $k$-mer counts of the string.
In this section, we will first model the $k$-mer count distribution of random strings.
We will then describe the complexity scoring function and the condition on bounding LC substrings in a long string.
We will compare our method to SDUST in the end.

\subsection{Notations}

Let $\Sigma=\{{\tt A},{\tt C},{\tt G},{\tt T}\}$ be the DNA alphabet,
$x\in\Sigma^*$ is a DNA string and $|x|$ is its length.
$t\in\Sigma^k$ is a $k$-mer.
For $|x|\ge k$, $c_x(t)$ is the occurrence of $k$-mer $t$ in $x$;
$\ell(x)=\sum_t c_x(t)=|x|-k+1$ is the total number of $k$-mers in $x$.
$\vec{c}_x$ denotes the count array over all $k$-mers.

In this article, we assume there is one long genome string of length $L$.
We use closed interval $[i,j]$ to denote the substring starting at $i$
and ending at $j$, including the end points $i$ and $j$.
We may use ``interval'' and ``substring'' interchangeably.

\subsection{Modeling $k$-mer counts}

Suppose symbols in $\Sigma$ all occur at equal frequency.
Then for all $k$-mer $t$, $c_x(t)\sim{\rm Poisson}(\lambda)$ where $\lambda=\ell(x)/4^k$.
Let
$$
p(n|\lambda)\triangleq\frac{\lambda^n}{n!}e^{-\lambda}
$$
be the probability mass function of Poisson distribution.
Notably, although $c_x(t)\le\ell(x)$, given that $\ell(x)\gg1$ in practice,
$$
p(\ell|\lambda)\approx\frac{e^{-\lambda}}{\sqrt{2\pi\ell}}\cdot\left(\frac{e}{4^k}\right)^{\ell}\ll 1
$$
with the Sterling formula -- $p(\ell|\lambda)$ is very close to 0.
This suggests Poisson remains a good approximation.
It is possible to start the derivation with a binomial distribution

The composite probability of string $x$ can be modeled by
\begin{equation}\label{eq:P0}
P(\vec{c}_x)=\prod_{t\in\Sigma^k}p(c_x(t)|\lambda)
\end{equation}
We have
\begin{equation}\label{eq:P}
\log P(\vec{c}_x)=4^k\lambda(\log\lambda-1)-\sum_t\log c_x(t)!
\end{equation}
The derivation from Eq.~(\ref{eq:P0}) to Eq.~(\ref{eq:P}) is exact.
It is possible to derive Eq.~(\ref{eq:P}) starting from a binomial distribution, instead of Poisson,
but we would introduce other approximation to reach a convenient form for implementation.

To get an intuition about $P(\vec{c}_x)$, suppose $\ell(x)\ll4^k$.
In this case, $c_x(t)$ will be mostly 0 or 1 for a random string
and the last term in Eq.~(\ref{eq:P}) will be close to 0.
Given an LC string of the same length,
we will see more $c_x(t)$ of 2 or higher, which will reduce $\log P(\vec{c}_x)$.
Thus the probability of an LC string is lower under this model.

Although $\log P(\vec{c}_x)$ can be used to compare the complexity of strings of the same length,
it does not work well for strings of different lengths because $\log P(\vec{c}_x)$ decreases with $\ell(x)$.
We would like to scale it to $Q(\vec{c}_x)$ such that
$Q$ approaches 0 given a random string.
We note that on the assumption of equal base frequency,
the average of $\log P(\vec{c}_x)$ can be approximated to
\begin{eqnarray*}
H(\lambda)&\triangleq&\sum_{\vec{c}}P(\vec{c})\cdot\sum_t\log p(c_t|\lambda)\\
&=&4^k\sum_{n=0}^{\infty}p(n|\lambda)\log p(n|\lambda)\\
&=&4^k\lambda(\log\lambda-1)-4^ke^{-\lambda}\sum_{n=0}^{\infty}\log n!\cdot\frac{\lambda^n}{n!}
\end{eqnarray*}
which is the negative entropy of $P$.
We can thus define
$$
Q(\vec{c}_x)\triangleq H(\lambda)-\log P(\vec{c}_x)=\sum_t\log c_x(t)!-f\left(\frac{\ell(x)}{4^k}\right)
$$
where
\begin{equation}\label{eq:f}
f(\lambda)\triangleq4^ke^{-\lambda}\sum_{n=0}^\infty\log n!\cdot\frac{\lambda^n}{n!}
\end{equation}
$Q(\vec{c}_x)$ is higher for LC string $x$.

\subsection{Scoring low-complexity intervals}

%As base frequencies vary with the GC content
%and genome sequences are rarely random,
%$Q(\vec{c}_x)$ still increases with $\ell(x)$ on real data.
To put a threshold on the complexity, we finally use the following function to score string complexity:
\begin{equation}\label{eq:S}
S(\vec{c}_x)\triangleq Q(\vec{c}_x)-T\cdot\ell(x)=\sum_t\log c_x(t)!-T\cdot\ell(x)-f\left(\frac{\ell(x)}{4^k}\right)
\end{equation}
Threshold $T$ controls the level of complexity in the output.
It defaults to $0.6$, less than $\log 2$.
At $k=7$, $f(\ell/4^k)$ is close to 0 for small $\ell$.
$0.6\ell$ is the larger term within the default window size of 5kb (Fig.~\ref{fig:f}).

\begin{figure}[b]
\centering
\includegraphics[width=0.7\columnwidth]{f-func}
\caption{Functions related to string complexity scoring in Eq.~(\ref{eq:S}).}\label{fig:f}
\end{figure}

If $S(\vec{c}_x)>0$, $x$ is considered to contain an LC substring.
However, we often do not want to classify the entire $x$ as an LC substring in this case
because the concatenation of a highly repetitive sequence and a random sequence
could still lead to a positive score.

Recall that we may use close intervals to represent substrings.
For convenience, we write $S(\vec{c}_{[i,j]})$ as $S(i,j)$.
In implementation, we precalculate $f(\ell/4^k)$ and introduce
\begin{eqnarray*}
U(i,j)&\triangleq&\sum_t\log c_{[i,j]}(t)!-T\cdot\ell([i,j])\\
&=&U(i,j-1)+\log c_{[i,j]}([j-k+1,j])-T
\end{eqnarray*}
We can thus compute the complexity scores of all prefixes of $[i,j]$
by scanning each base in the interval from left to right;
we can similarly compute all suffix scores from right to left.

\subsection{Finding low-complexity regions}

We say $x$ is a \emph{perfect LC string} (or \emph{perfect LC interval})
if $S(\vec{c}_x)>0$ and no substring of $x$ is scored higher than $S(\vec{c}_x)$;
say $x$ is a \emph{good LC string} (or \emph{good LC interval})
if $S(\vec{c}_x)>0$ and no prefix or suffix of $x$ is scored higher than $S(\vec{c}_x)$.
We can use $U(i,j)$ above to test if $[i,j]$ is a good LC interval in linear time.
If we apply this method to all intervals up to $w$ in length (5000bp by default),
we can find LC regions of context length up to $w$ in $O(w^2L)$ time.
The union of all good LC intervals marks the LC regions in a genome.

\begin{algorithm}[bt]
	\caption{Find LC interval ending at $j$}\label{algo:LC1}
	\begin{algorithmic}[1]
		\Procedure{FindStart}{$k,w,T,j,c'$}
			\State $B\gets${\sc Backward}$(k,w,T,j,c')$
			\State $j'_{\max}\gets-1$
			\For{$(i,v')\in B$ in the ascending order of $i$}
				\State {\bf continue if} $i<j'_{\max}$\Comment{this is an approximation}
				\State $j'\gets${\sc Forward}$(k,T,i,j,v')$
				\State \Return $i$ {\bf if} $j'=j$\Comment{$[i,j]$ is a good LC interval}
				\State $j'_{\max}\gets\max(j'_{\max},j')$
			\EndFor
			\State \Return $-1$\Comment{No good LC interval ending at $j$}
		\EndProcedure
		\Procedure{Backward}{$k,w,T,j,c'$}
			\State $u\gets 0$; $v_0\gets-1$; $u'\gets 0$
			\State $v_{\max}\gets0$; $i_{\max}\gets-1$; $c\gets[0]$
			\State $B\gets\emptyset$
			\For{$i\gets j$ {\bf to} $\max(j-w+1,k-1)$}\Comment{$i$ is descending}
				\State $t\gets[i-k+1,i]$\Comment{the $k$-mer ending at $i$}
				\State $c[t]\gets c[t]+1$
				\State $u\gets u+\log(c[t])-T$
				\State $v\gets u-f((j-i+1)/4^k)$\Comment{$v=S(i-k+1,j)$}
				\If{$v<v_0$ {\bf and} $v_0=v_{\max}$}
					\State $B\gets B\cup\{(i+1,v_{\max})\}$\Comment{a candidate start pos}
				\ElsIf{$v\ge v_{\max}$}
					\State $v_{\max}\gets v$; $i_{\max}\gets i$
				\ElsIf{$i_{\max}<0$}
					\State $u'\gets u'+\log(c'[t])-T$\Comment{$c'[t]\triangleq c_{[j-w+1,j]}(t)$}
					\State {\bf break if} $u'<0$\Comment{{\sc Forward}() wouldn't reach $j$}
				\EndIf
				\State $v_0\gets v$
			\EndFor
			\State $B\gets B\cup\{(i_{\max},v_{\max})\}$ {\bf if} $i_{\max}\ge0$
			\State\Return $B$
		\EndProcedure
		\Procedure{Forward}{$k,T,i_0,j,v'_{\max}$}
			\State $u\gets 0$; $v_{\max}\gets0$; $i_{\max}\gets-1$; $c\gets[0]$
			\For{$i\gets i_0$ {\bf to} $j$}
				\State $t\gets[i-k+1,i]$
				\State $c[t]\gets c[t]+1$
				\State $u\gets u+\log(c[t])-T$
				\State $v\gets u-f((i-i_0+1)/4^k)$
				\If{$v\ge v_{\max}$}
					\State $v_{\max}\gets v$; $i_{\max}\gets i$
				\EndIf
				\State {\bf break if} $v>v'_{\max}$
			\EndFor
			\State\Return $i_{\max}$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Algorithm~\ref{algo:LC1} shows a faster way to find a good LC interval ending at $j$.
Function {\sc Backward}() scans backwardly from $j$ to $j-w+1$ to collect candidate start positions (line 22).
Variable $v$ is the complexity score of suffix $[i-k+1,j]$ (line 20).
By the definition of good LC interval, $i$ can only be a candidate start if $v$ is no less than all the suffixes visited before (line 21).
We also ignore a candidate start $i$ if $S(i,j)<S(i-1,j)$
because if $[i-1,j]$ is not a good LC interval, there must exist $i'>i$ such that $S(i',j)>S(i-1,j)<S(i,j)$, so $[i,j]$ would not be a good LC interval, either.
In addition, if suffix $[i,j]$ is enriched with $k$-mers unique in the full window $[j-w+1,j]$,
$[i,j]$ will not be a good LC interval (line 27) as there will exist $j'<j$ such that $S(i,j')>S(i,j)$.
The time complexity of {\sc Backward}() is $O(w)$.

Given a candidate start position $i$,
function {\sc Forward}() returns $j'=\argmax_{i<j'\le j} S(i,j')$.
$[i,j]$ will be a good LC interval if and only if $j'=j$ (Line 7).
We call {\sc Forward}() in the ascending order of candidate start positions (line 4).
We may skip a start position if it is contained in an interval found from previous {\sc Forward}() calls (line 5).
This is an approximation as it is possible for a good LC interval to start in another good interval.
An alternative heuristic is to only apply {\sc Forward}() to the smallest candidate start in $B$.
This leads to a guaranteed $O(w)$ with {\sc FindStart}().
In practice, the two algorithms have almost identical runtime.
We use Algorithm~\ref{algo:LC1} in longdust as it is closer to the exact algorithm.

Function {\sc FindStart}() finds the longest good LC interval ending at one position.
We apply the function to every position in the genome to find all good LC intervals.
We can skip $j$ if $[j-k+1,j]$ is unique in $[j-w+1,j]$ because the forward pass would not reach $j$ in this case.
We also introduce a heuristic to extend a good LC interval $[j-w,j-1]$ to $[j-w+1,j]$ without calling {\sc FindStart}().

Our definition may classify a short non-repetitive interval to LC if it is flanked by highly repetitive LC intervals.
We additionally use an X-drop heuristic~\citep{Altschul:1997vn} to alleviate this case.
On the T2T-CHM13 genome, the total length of LC intervals is 0.5\% shorter with X-drop.
The heuristic has a minor effect.

The overall longdust algorithm is inexact and may result in slightly different LC regions on opposite strands (21kb out of 278Mb in T2T-CHM13).
We run the algorithm on both the forward and the reverse strand of the input sequences and merge the resulting intervals.
The default longdust output is strand symmetric.

\subsection{Adjusting for non-equal $k$-mer frequencies}

We assume each $k$-mer $t$ occurs at frequency $4^{-k}$ when deriving Eq.~(\ref{eq:S}).
It is possible to relax this assumption.
Let $q_t$ be the expected frequency of $t$ with $\sum_{t\in\Sigma^k}q_t=1$.
For convenience, introduce $r_t\triangleq4^kq_t$.
Under equal frequency, $q_t=4^{-k}$ and thus $r_t=1$.
For general $q_t$, Eq.~(\ref{eq:P}) becomes
$$
\log P(\vec{c}_x)=4^k\lambda(\log\lambda-1)-\sum_{t\in\Sigma^k}\log c_x(t)!+\sum_{t\in\Sigma^k}c_x(t)\log r_t
$$
The revised $H(\lambda)$ is more involved:
\begin{eqnarray*}
H(\lambda)&=&\sum_t\sum_{n=0}^{\infty}p(n|\lambda r_t)\log p(n|\lambda r_t)\\
&=&4^k\lambda(\log\lambda-1)+\lambda\sum_t r_t\log r_t\\
&&-\sum_t e^{-\lambda r_t}\sum_{n=0}^{\infty}\log n!\cdot\frac{(\lambda r_t)^n}{n!}
\end{eqnarray*}
We can similarly define the scoring function:
\begin{eqnarray*}
S(\vec{c}_x)&=&H(\lambda)-\log P(\vec{c}_x)-T\cdot\ell(x)\\
&=&\sum_t\log c_x(t)!-\sum_tc_x(t)\log r_t-T'\cdot\ell(x)-f(4^{-k}\ell(x))
\end{eqnarray*}
where
$$
f(\lambda)=\sum_t e^{-\lambda r_t}\sum_{n=0}^{\infty}\log n!\cdot\frac{(\lambda r_t)^n}{n!}
$$
and
$$
T'=T-4^{-k}\sum_t r_t\log r_t=T-\sum_t q_t\log r_t
$$
The last term in $T'$ is the relative entropy between $\{q_t\}$ and the uniform distribution.
It is thus always non-negative.
This term reduces the score threshold, making it easier to call LC intervals.

\subsection{Comparison to SDUST}

SDUST~\citep{Morgulis:2006aa} uses the following complexity scoring function:
$$
S_S(\vec{c}_x)=\frac{1}{\ell(x)}\sum_t\frac{c_x(t)(c_x(t)-1)}{2}-T
$$
This function grows linearly with $\ell(x)$ for $\ell(x)\ge4^k$,
while our scoring function grows more slowly in the logarithm scale.
The SDUST function is more likely to classify longer sequences as LC.

Furthermore, SDUST looks for perfect LC intervals rather than good LC intervals like longdust.
It cannot test whether an interval is perfect in linear time.
Instead, SDUST maintains the complete list of perfect intervals in window $[j-w+1,j]$
and tests a new candidate interval against the list.
The {\sc FindStart}() equivalent of SDUST is $O(w^3)$ in time, impractical for long windows.
SDUST hardcodes $k=3$ and uses $w=64$ by default for acceptable performance.

\subsection{Scoring with Shannon entropy}

Let $p_x(t)=c_x(t)/\ell(x)$.
The Shannon entropy of string $x$ is
$$
H(\vec{c}_x)\triangleq-\sum_tp_x(t)\log p_x(t)=\log\ell(x)-\frac{1}{\ell(x)}\sum_t c_x(t)\log c_x(t)
$$
When $\ell(x)\le4^k$, $H(\vec{c}_x)$ reaches the maximum value of $\log\ell(x)$ at $p_x(t)=1/\ell(x)$.
$H(\vec{c}_x)$ also grows with $\ell(x)$.
For $\ell(x)\le4^k$, define
$$
S_E(\vec{c}_x)\triangleq\log\ell(x)-H(\vec{c}_x)-T=\frac{1}{\ell(x)}\sum_t c_x(t)\log c_x(t)-T
$$
We adapted longdust for $S_E$ and found using $S_E$ is more than twice as slow.
We suspected some longdust heuristics did not work well with $S_E$, but we did not investigate further.

\section{Results}

\subsection{Low-complexity regions in T2T-CHM13}

\begin{table}[tb]
\caption{Command lines and resource usage for T2T-CHM13\label{tab:cmd}}
\begin{tabular*}{\columnwidth}{@{\extracolsep\fill}lrrl@{\extracolsep\fill}}
\toprule
Tool & $t_{\rm CPU}$ (h) & Mem (G) & Command line \\
\midrule
longdust & 0.94   & 0.47 & (default) \\
SDUST    & 0.07   & 0.23 & {\tt -t30} \\
pytrf    & 2.64   & 0.70 & {\tt -M500} \\
TRF      & 12.83  & 7.49 & {\tt 2 7 7 80 10 50 500 -l12} \\
TANTAN   & 0.56   & 1.28 & {\tt -w500} \\
ULTRA    & 146.07 & 33.31& {\tt -p500 -t16} \\
\botrule
\end{tabular*}
\begin{tablenotes}\setlength\itemsep{0.0em}
Performance measured on a Linux server equipped with Intel Xeon Gold 6130 CPU and 512GB memory.
\end{tablenotes}
\end{table}

\begin{figure}[tb]
\includegraphics[width=\columnwidth]{len}
\caption{Lengths of low-complexity regions in T2T-CHM13.
Low-complexity (LC) regions identified by each tool are first intersected with centromeric satellite annotation.
The remainder is then intersected with longdust, TRF and SDUST in order.
There are no overlaps between stacks.
The total height is the length of LC regions found by each tool.
Alternative settings --
``longdust-w8k'': window size $w=8000$;
``longdust-t.5'': score threshold $T=0.5$;
``longdust-k8'': using 8-mers;
``TRF-c4'': requiring $\ge4$ copies of the repeat unit;
``TANTAN-r1e-9'': run with ``-r 1e-9'';
``ULTRA-s30'': requiring score $\ge30$ in the output.
}\label{fig:len}
\end{figure}

We applied longdust, SDUST v0.1~\citep{Morgulis:2006aa},
pytrf v1.4.2~\citep{Du:2025aa},
TRF v4.10~\citep{Benson:1999aa},
TANTAN v51~\citep{Frith:2011aa},
and ULTRA v1.20~\citep{Olson:2024aa}
to the T2T-CHM13 human genome~\citep{Nurk:2022up}.
The maximum period was set to 500 (Table~\ref{tab:cmd}).
Notably, TRF would not finish in days with the default option {\tt -l}.
With {\tt -l30}, TRF could run in 11 hours using 17.82GB memory at the peak.

Due to the lack of ground truth, we could not calculate the accuracy of longdust.
We instead checked how often longdust reports regions unique to itself
and how often it misses regions reported by other tools, especially by two or more of them.

Longdust finds 277.1Mb of LC regions with 224.3Mb overlapping with centromeric satellite annotated by the telomere-to-telomere (T2T) consortium (Fig.~\ref{fig:len}).
Of the remaining 52.7Mb, 34.1Mb overlaps with TRF; 15.4Mb of the remainder (18.6Mb) is found by SDUST.
Only 3.2Mb is left, suggesting most longdust LC regions fall in centromeres or are found by TRF or SDUST.
Longdust uses three parameters: $k$-mer size, window size $w$ and score threshold $T$.
Varying these parameters do not greatly alter the results.

TRF, the most popular tandem repeat finder, finds 274.5Mb of tandem repeats,
244.0Mb of which have $\ge4$ copies of repeat units.
97.9\% of the 244.0Mb are identified by longdust.
TRF additionally reports tandem repeats with $<4$ repeat units.
Only 14.8\% of them overlap with longdust results.
Longdust misses repeats with low copy numbers.
In fact, under the default threshold $T=0.6$,
the minimum numbers of exact copies longdust can find is approximately:
$$
3+\frac{k-1}{r}+\frac{3T-\log2-\log3}{\log4-T}\approx3.01+\frac{k-1}{r}
$$
This assumes the repeat unit length $r>k$, $f(\cdot)=0$ and all $k$-mers are unique within the repeat unit.

ULTRA outputs 68.2Mb of regions not reported by longdust, TRF or SDUST.
Nevertheless, if we raise its score threshold to 30,
the total region length is similar to that of TRF
with most of repeats of $\ge$4 copies captured by longdust.
Under the default setting, TANTAN finds the largest LC regions missed by longdust, TRF or SDUST.
Decreasing the repeat starting threshold from 0.005 to $10^{-9}$ greatly reduces its unique regions from 148.5Mb to 24.5Mb.
We suspect the remaining regions unique to TANTAN may have low copy number but we cannot control its output.
TANTAN can optionally generate repeat units but this mode is six times as slow and reports only 27\% of regions in the default mode.

As to other tools, pytrf cannot effectively identify alpha satellite with $\sim$170bp repeat units,
even though it was set to find tandem repeats with unit up to 500bp.
With a 64bp window size, SDUST naturally misses tandem repeats with long units, including all alpha satellite.

\subsection{Behaviors in long satellite repeats}

Although longdust, TRF, TANTAN and ULTRA report similar amount of human satellites,
the LC block sizes vary greatly.
We will take the centromeric satellite of chromosome 11 for an example.
Around this centromere, longdust reports a continuous block of 3.5Mb from 50,999,020 to 54,488,059 on T2T-CHM13.
TRF finds a nearly identical region, with the ending position shifted by only 3bp.
In contrast, although ULTRA finds 99.86\% of this region,
it breaks them into many smaller blocks.
The longest block is 120kb in size.
TANTAN under the default setting misses 2.3\% of this long alpha array and its output is more fragmented.
The longest block is only 1587bp.
With ``-r 1e-9'', TANTAN misses 7.6\% of this satellite.
Across all satellite regions annotated on T2T-CHM13,
the N50 block size is 2.9Mb, 2.0Mb, 10.2kb and 340bp for longdust, TRF, ULTRA and TANTAN, respectively.
Longdust reports the longest blocks probably because it merges satellites of different types,
which is a weakness in comparison to TRF.

One application of LC masking is to speed up cross-species whole-genome alignment.
90.8\% of satellites found by longdust lie in blocks longer than 10kb
and conversely, 99.2\% of blocks longer than 10kb fall in annotated satellites.
By setting a block length threshold, we can mask the bulk of satellite without affecting the rest of the genome.
A similar strategy works for TRF that also reports long blocks.
We would need to stitch shorter blocks reported ULTRA and TANTAN to use this strategy,
which would add another layer of complexity.

\subsection{Low-complexity regions in a gorilla genome}

The near T2T gorilla genome (AC:GCF\_029281585.2; \citealp{Yoo:2025aa}) is 3546Mb in size, 428Mb larger than the human T2T-CHM13 genome.
We ran longdust on the gorilla genome for 1.4 hours and found 656.8Mb of LC regions.
That is 379.7Mb larger than the LC regions in T2T-CHM13.
The genome size difference is primarily driven by LC regions.

To further confirm this observation,
we extracted 298.8Mb of regions in the gorilla genome that are $\ge10$kb in length without any 51-mer exact matches to 472 human genomes~\citep{Li:2024ac}.
99.7\% of them are marked as LC regions by longdust and none of them are alpha satellite.
95.8\% of these gorilla-specific regions are distributed within 15Mb from telomeres,
broadly in line with \citet{Yoo:2025aa}.
We also ran TRF on the gorilla genome.
It did not finish in a week with option ``{\tt -l20}''.

\section{Discussions}

Implemented in the C programming language, longdust is a fast and lightweight command-line tool for
identifying low-complexity regions.
It rarely finds LC regions not reported by TRF plus SDUST
and can recover most tandem repeats with $\ge4$ copies of repeat units.
Longdust provides basic APIs in C and can also be used as a programming library.

From the theoretical point of view, longdust uses an approximate algorithm.
It tests LC intervals ending at each position with Algorithm~\ref{algo:LC1},
but has not sufficiently exploited dependencies between positions.
It will be interesting to see whether there is an exact $O(wL)$ algorithm under the longdust formulation
or a meaningful alternative formulation that leads fast implementations.

In comparison to tandem repeat finding tools such as TRF and ULTRA,
longdust is unable to report the repeat units in case of tandem repeats.
This will limit the use cases of longdust.
Furthermore, unlike TANTAN and ULTRA that model extreme GC content,
longdust assumes the four types of nucleotides occurring at equal frequency.
This will lead to enriched LC regions in AT- or GC-rich genomes.
A solution is to introduce $q_t$ as the expected frequency of $k$-mer $t$.
Eq.~(\ref{eq:P}) will become
$$
\log P(\vec{c}_x)=4^k\lambda(\log\lambda-1)-\sum_{t\in\Sigma^k}\log c_x(t)!+\sum_{t\in\Sigma^k}c_x(t)\log(4^kq_t)
$$

A potential heuristic is to weight the count of $k$-mer $t$ by $t$'s GC content in Eq.~(\ref{eq:P}).
We have not explored this strategy.

Another practical limitation of longdust is the restricted window size.
The genome of Woodhouse's scrub jays, for example, contains satellite with a 18kb repeat unit~\citep{Edwards:2025aa}.
This would be missed by longdust under the default setting.
Increasing the window size would make longdust considerably slower.
This is partly due to the $O(wL)$ time complexity and partly due to the speedup strategies in longdust that are more effective given $\ell(x)\ll 4^k$.
It would be ideal to have an algorithm that remains efficient given large windows or, better, does not require specified window sizes.

\section*{Acknowledgments}

We are grateful to Qian Qin for evaluating the effect of low-complexity regions in structural variant calling,
and to Maximilian Haeussler for explaining the role of masking low-complexity regions in cross-species alignment.
We thank one of the reviewers for suggesting better parameters.

\section*{Author contributions}

H.L. conceived the project, implemented the method, analyzed the data and drafted the manuscript.
B.L. prototyped the algorithm.

\section*{Conflict of interest}

None declared.

\section*{Funding}

This work is supported by US National Institute of Health grant R01HG010040, R01HG014175, U01HG013748, U41HG010972, and U24CA294203 (to H.L.).

\section*{Data availability}

\url{https://github.com/lh3/longdust}

\bibliographystyle{apalike}
{\sffamily\small
\bibliography{longdust}}

\end{document}
